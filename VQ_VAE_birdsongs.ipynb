{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c763a1d-2f77-4315-bd1b-12bde322978e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78208898-41e5-490c-a105-aea8b2c24227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:03:44.768336: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752361424.912680  120968 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752361424.949403  120968 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752361425.128709  120968 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752361425.128745  120968 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752361425.128749  120968 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752361425.128753  120968 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-12 19:03:45.157308: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from keras.saving import register_keras_serializable\n",
    "from umap import UMAP\n",
    "from tensorflow.keras.models import load_model\n",
    "import keras as K  # deconflict with tensorflow.keras\n",
    "\n",
    "\n",
    "SPECTROGRAM_PATH=\"/mnt/md0/spectrograms\"  # output\n",
    "fnames = glob(\"../birdsongs*/**/*mp3\")    # input\n",
    "random.shuffle(fnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a5bde2-3a36-4e51-b66d-26c730305800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spectrograms(f):\n",
    "    w = []\n",
    "    THRESHOLD = 1000  # pretty much useless right now\n",
    "    HOPLENGTH = 100\n",
    "    STRIDE = 50\n",
    "    x,sr = librosa.load(f)\n",
    "    y = librosa.feature.melspectrogram(y=x, n_mels=224,n_fft=4096*2, hop_length=HOPLENGTH)\n",
    "    y=y-y.min() + 1e-5\n",
    "    y=y/y.max()\n",
    "    y=np.log(np.abs(y))  # these will probably have range from -11 to 0\n",
    "    y = y -y.min()\n",
    "    y = y/y.max()\n",
    "    z =[y[:, i:i+224] for i in range(0, y.shape[1]-224, STRIDE)]\n",
    "    w.append(np.stack(z))\n",
    "    w = np.vstack(w)\n",
    "    w= np.expand_dims(w, -1) # (batch, 224,224,1)\n",
    "\n",
    "    filtered_w = []\n",
    "    energies = []\n",
    "    for img in w:\n",
    "        energy = np.sum(img**2)\n",
    "        energies.append(energy)\n",
    "        if(energy >THRESHOLD):\n",
    "            filtered_w.append(img)\n",
    "    w = np.array(filtered_w)\n",
    "    for i, ww in enumerate(w):\n",
    "        if(ww.shape==(224,224,1)):\n",
    "            fname = f\"{SPECTROGRAM_PATH}/{Path(f).stem}_{i}\"\n",
    "            arr = (ww - ww.min()) / (ww.max() - ww.min())\n",
    "            np.save(fname, arr)\n",
    "     \n",
    "    return\n",
    "\n",
    "for f in tqdm(fnames[:1200]):\n",
    "    try:\n",
    "        make_spectrograms(f)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae82e67-8e00-449d-893e-2b219d945e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = np.load(glob(f\"{SPECTROGRAM_PATH}/*npy\")[0])\n",
    "x.max(), x.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d9fbdc-51be-4eb7-8ba7-bc5707cd5ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe539144-7ae5-4bb0-bb10-64c3cc5e3918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4782"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class AutoencoderDataGenerator(Sequence):\n",
    "    def __init__(self, fnames, batch_size, shuffle=True):\n",
    "        \"\"\"\n",
    "        Initialize the data generator.\n",
    "        \n",
    "        Args:\n",
    "            fnames (list): List of file paths to .npy files\n",
    "            batch_size (int): Number of samples per batch\n",
    "            shuffle (bool): Whether to shuffle the data after each epoch\n",
    "        \"\"\"\n",
    "        self.fnames = fnames\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(fnames))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of batches per epoch.\"\"\"\n",
    "        return int(np.ceil(len(self.fnames) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data.\"\"\"\n",
    "        # Get indices for the current batch\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        \n",
    "        # Get list of filenames for the batch\n",
    "        batch_fnames = [self.fnames[i] for i in batch_indices]\n",
    "        \n",
    "        # Load data\n",
    "        X = self._load_data(batch_fnames)\n",
    "        \n",
    "        # Return (x, x) for autoencoder\n",
    "        return X, X\n",
    "\n",
    "    def _load_data(self, batch_fnames):\n",
    "        \"\"\"Load and preprocess data from .npy files.\"\"\"\n",
    "        # Initialize list to store batch data\n",
    "        batch_data = []\n",
    "        \n",
    "        # Load each .npy file\n",
    "        for fname in batch_fnames:\n",
    "            # Load numpy array\n",
    "            data = np.load(fname)\n",
    "            batch_data.append(data)\n",
    "        \n",
    "        # Stack arrays into a batch (shape: (batch_size, 224, 224, 1))\n",
    "        batch_data = np.stack(batch_data)\n",
    "        \n",
    "        # Optional: Normalize data to [0, 1] if needed\n",
    "        # batch_data = batch_data / 255.0  # Uncomment if your data needs normalization\n",
    "        \n",
    "        return batch_data\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Shuffle indices after each epoch if shuffle is True.\"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "data_gen = AutoencoderDataGenerator(glob(\"/mnt/md0/spectrograms/*npy\"), 64, )\n",
    "data_gen.num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca69ab2-5132-4dbc-a5bc-5ee159303055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c37ed97-ca50-409f-a93a-0641fa5b9bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,x = data_gen.__getitem__(100)\n",
    "x[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0225beeb-e752-4a32-8d22-7da94263e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a few grams\n",
    " \n",
    "images = x[:20,...]\n",
    "fig, axes = plt.subplots(5, 4, figsize=(8, 10)) # 5 rows, 4 columns, adjust figsize as needed\n",
    "axes = axes.flatten() # Flatten the 2D array of axes for easy iteration\n",
    "\n",
    "# Iterate through images and display them\n",
    "for i, img in enumerate(images):\n",
    "    axes[i].imshow(img, ) # Using grayscale colormap for dummy images\n",
    "    axes[i].axis('off') # Turn off axes for a clean display\n",
    "    axes[i].set_title( str(np.sum(img**2)))\n",
    "# Adjust layout for a tight display\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1b1cf9a-f622-41ff-bc48-ba4f7524ff1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1752361454.030714  120968 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10690 MB memory:  -> device: 0, name: NVIDIA TITAN V, pci bus id: 0000:01:00.0, compute capability: 7.0\n",
      "I0000 00:00:1752361454.031299  120968 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 10690 MB memory:  -> device: 1, name: NVIDIA TITAN V, pci bus id: 0000:04:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "VQ-VAE for 224×224×1 spectrograms\n",
    "=================================\n",
    "Stable training with:\n",
    "  * EMA Vector-Quantiser\n",
    "  * MSE reconstruction loss\n",
    "  * Gradient-norm clipping\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "try:\n",
    "    import tensorflow_addons as tfa          # for GroupNorm\n",
    "    GroupNorm = lambda: tfa.layers.GroupNormalization(groups=8)\n",
    "except ImportError:\n",
    "    GroupNorm = lambda: layers.BatchNormalization()\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "# 1.  Vector-Quantiser, EMA version (no trainable parameters)\n",
    "# --------------------------------------------------------------------- #\n",
    "@register_keras_serializable()\n",
    "class VectorQuantizerEMA(layers.Layer):\n",
    "    def __init__(self,\n",
    "                 num_embeddings: int,\n",
    "                 embedding_dim : int,\n",
    "                 commitment_cost: float = 0.25,\n",
    "                 decay: float = 0.99,\n",
    "                 epsilon: float = 1e-5,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_embeddings  = num_embeddings\n",
    "        self.embedding_dim   = embedding_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.decay           = decay\n",
    "        self.epsilon         = epsilon\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Code-book\n",
    "        init = tf.random.uniform([self.num_embeddings, self.embedding_dim],\n",
    "                                 minval=-1., maxval=1.)\n",
    "        self.embeddings      = tf.Variable(init, trainable=False,\n",
    "                                           name='embeddings')\n",
    "        # EMA helpers\n",
    "        self.ema_cluster_size = tf.Variable(tf.zeros([self.num_embeddings]),\n",
    "                                            trainable=False)\n",
    "        self.ema_embed = tf.Variable(init, trainable=False)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs: [B, H, W, D]\n",
    "        input_shape = tf.shape(inputs)\n",
    "        flat_inputs = tf.reshape(inputs, [-1, self.embedding_dim])   # [BHW,D]\n",
    "\n",
    "        # Squared Euclidean distance to every embedding\n",
    "        distances = (tf.reduce_sum(flat_inputs**2, 1, keepdims=True)\n",
    "                     - 2 * tf.matmul(flat_inputs, self.embeddings, transpose_b=True)\n",
    "                     + tf.reduce_sum(self.embeddings**2, 1))\n",
    "\n",
    "        encoding_indices = tf.argmin(distances, axis=1)              # [BHW]\n",
    "        encodings = tf.one_hot(encoding_indices, self.num_embeddings)  # [BHW,K]\n",
    "\n",
    "        # Quantise and reshape back to image\n",
    "        quantised = tf.matmul(encodings, self.embeddings)            # [BHW,D]\n",
    "        quantised = tf.reshape(quantised, input_shape)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # EMA update of code-book (executed only in training mode)\n",
    "        # ------------------------------------------------------------------\n",
    "        if training:\n",
    "            updated_cluster_size = tf.reduce_sum(encodings, axis=0)          # [K]\n",
    "            dw = tf.matmul(encodings, flat_inputs, transpose_a=True)         # [K,D]\n",
    "\n",
    "            self.ema_cluster_size.assign(self.decay * self.ema_cluster_size +\n",
    "                                         (1. - self.decay) * updated_cluster_size)\n",
    "            self.ema_embed.assign(self.decay * self.ema_embed +\n",
    "                                  (1. - self.decay) * dw)\n",
    "\n",
    "            n = tf.reduce_sum(self.ema_cluster_size)            # total elements\n",
    "            cluster_size = ((self.ema_cluster_size + self.epsilon) /\n",
    "                            (n + self.num_embeddings * self.epsilon)) * n\n",
    "            normalized_embed = self.ema_embed / tf.expand_dims(cluster_size, 1)\n",
    "            self.embeddings.assign(normalized_embed)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # Losses\n",
    "        # ------------------------------------------------------------------\n",
    "        commitment_loss = tf.reduce_mean(\n",
    "            tf.square(tf.stop_gradient(quantised) - inputs))\n",
    "        self.add_loss(self.commitment_cost * commitment_loss)\n",
    "\n",
    "        # Straight-through estimator\n",
    "        quantised = inputs + tf.stop_gradient(quantised - inputs)\n",
    "        return quantised, tf.reshape(encoding_indices, input_shape[:3])  # indices for later use\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "# 2.  Encoder / Decoder building blocks\n",
    "# --------------------------------------------------------------------- #\n",
    "def enc_block(x, filters, stride=2):\n",
    "    x = layers.Conv2D(filters, 4, strides=stride,\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    x = GroupNorm()(x)\n",
    "    return layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "def dec_block(x, filters):\n",
    "    x = layers.UpSampling2D()(x)\n",
    "    x = layers.Conv2D(filters, 3, padding='same', use_bias=False)(x)\n",
    "    x = GroupNorm()(x)\n",
    "    return layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "# 3.  Complete VQ-VAE model\n",
    "# --------------------------------------------------------------------- #\n",
    "def build_vqvae(input_shape=(224, 224, 1),\n",
    "                embedding_dim=32,\n",
    "                num_embeddings=512,\n",
    "                beta=0.25):\n",
    "    # ---------------- Encoder ---------------- #\n",
    "    encoder_in  = keras.Input(shape=input_shape)\n",
    "    x = enc_block(encoder_in,  32)    # 112×112\n",
    "    x = enc_block(x,  64)             # 56×56\n",
    "    x = enc_block(x, 128)             # 28×28\n",
    "    x = enc_block(x, 256)             # 14×14\n",
    "    x = layers.Conv2D(embedding_dim, 1, padding='same',\n",
    "                      use_bias=False)(x)          # (14,14,32)\n",
    "    encoder_out = x\n",
    "    encoder = keras.Model(encoder_in, encoder_out, name='encoder')\n",
    "\n",
    "    # -------------- Vector-Quantiser ---------- #\n",
    "    vq_layer = VectorQuantizerEMA(num_embeddings,\n",
    "                                  embedding_dim,\n",
    "                                  commitment_cost=beta,\n",
    "                                  name='vector_quantiser')\n",
    "\n",
    "    # --------------- Decoder ------------------ #\n",
    "    quant_in = keras.Input(shape=(14, 14, embedding_dim))\n",
    "    y = dec_block(quant_in, 256)      # 28×28\n",
    "    y = dec_block(y, 128)             # 56×56\n",
    "    y = dec_block(y,  64)             # 112×112\n",
    "    y = dec_block(y,  32)             # 224×224\n",
    "    logits = layers.Conv2D(1, 3, padding='same')(y)   # raw logits\n",
    "    decoder_out = layers.Activation('sigmoid')(logits)\n",
    "    decoder = keras.Model(quant_in, decoder_out, name='decoder')\n",
    "\n",
    "    # ------------- End-to-end Model ----------- #\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    z_e = encoder(inputs)\n",
    "    z_q, indices = vq_layer(z_e)\n",
    "    recon = decoder(z_q)\n",
    "\n",
    "    # ------------- End-to-end subclassed model -------------------- #\n",
    "    @register_keras_serializable()\n",
    "    class VQVAE(keras.Model):\n",
    "        def __init__(self, encoder, vq_layer, decoder, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.encoder  = encoder\n",
    "            self.vq_layer = vq_layer\n",
    "            self.decoder  = decoder\n",
    "\n",
    "            self.total_loss_tracker = keras.metrics.Mean(name='total_loss')\n",
    "            self.recon_loss_tracker = keras.metrics.Mean(name='recon_loss')\n",
    "            self.vq_loss_tracker    = keras.metrics.Mean(name='vq_loss')\n",
    "\n",
    "        # forward pass\n",
    "        def call(self, x, training=False):\n",
    "            z_e = self.encoder(x, training=training)\n",
    "            z_q, _ = self.vq_layer(z_e, training=training)\n",
    "            return self.decoder(z_q, training=training)\n",
    "\n",
    "        # custom training loop\n",
    "        def train_step(self, data):\n",
    "            if isinstance(data, tuple):\n",
    "                data = data[0]\n",
    "            with tf.GradientTape() as tape:\n",
    "                recon   = self(data, training=True)\n",
    "                r_loss  = tf.reduce_mean(tf.square(data - recon))\n",
    "                vq_loss = tf.add_n(self.vq_layer.losses)       # commitment term\n",
    "                t_loss  = r_loss + vq_loss\n",
    "\n",
    "            grads = tape.gradient(t_loss, self.trainable_variables)\n",
    "            grads = [tf.clip_by_norm(g, 1.0) if g is not None else None\n",
    "                     for g in grads]\n",
    "            self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "            self.total_loss_tracker.update_state(t_loss)\n",
    "            self.recon_loss_tracker.update_state(r_loss)\n",
    "            self.vq_loss_tracker.update_state(vq_loss)\n",
    "            return {'loss':        self.total_loss_tracker.result(),\n",
    "                    'recon_loss':  self.recon_loss_tracker.result(),\n",
    "                    'vq_loss':     self.vq_loss_tracker.result()}\n",
    "\n",
    "        # (encode / decode helpers exactly as before)\n",
    "        def encode(self, x):\n",
    "            z_e = self.encoder(x, training=False)\n",
    "            _, idx = self.vq_layer(z_e, training=False)\n",
    "            return idx                        # shape (B,14,14)\n",
    "\n",
    "        def decode_from_indices(self, indices):\n",
    "            one_hot = tf.one_hot(indices, depth=num_embeddings)      # [B,14,14,K]\n",
    "            flat    = tf.reshape(one_hot, [-1, num_embeddings])\n",
    "            z_q = tf.matmul(flat, self.vq_layer.embeddings)          # [B*14*14,D]\n",
    "            z_q = tf.reshape(z_q, [-1, 14, 14, embedding_dim])\n",
    "            return self.decoder(z_q, training=False)\n",
    "\n",
    "        def get_perplexity(self):\n",
    "            \"\"\"lower beta if perplexity is less than 10% (using few codes)\"\"\"\n",
    "            probs = self.ema_cluster_size / tf.reduce_sum(self.ema_cluster_size)\n",
    "            return tf.exp(-tf.reduce_sum(probs * tf.math.log(probs + 1e-10)))\n",
    "\n",
    "        \n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                \"encoder\": K.saving.serialize_keras_object(self.encoder),\n",
    "                \"vq_layer\": K.saving.serialize_keras_object(self.vq_layer),\n",
    "                \"decoder\": K.saving.serialize_keras_object(self.decoder)\n",
    "            })\n",
    "            return config\n",
    "            \n",
    "        @classmethod\n",
    "        def from_config(cls, config):\n",
    "            encoder = K.saving.deserialize_keras_object(config.pop(\"encoder\"))\n",
    "            vq_layer = K.saving.deserialize_keras_object(config.pop(\"vq_layer\"))\n",
    "            decoder = K.saving.deserialize_keras_object(config.pop(\"decoder\"))\n",
    "            return cls(encoder, vq_layer, decoder, **config)\n",
    "\n",
    "    # create and return the model (NO positional tensors)\n",
    "    return VQVAE(encoder, vq_layer, decoder, name='VQ_VAE')\n",
    "_ = build_vqvae(input_shape=(224,224,1),\n",
    "                            embedding_dim=32,\n",
    "                            num_embeddings=1024,\n",
    "                            beta=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb0bbb5-6499-4f1f-9342-8ddce404b00b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building new model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gary/miniconda3/envs/tf_gpu/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1752361470.040287  121040 service.cc:152] XLA service 0x7f2f0801bb90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1752361470.040309  121040 service.cc:160]   StreamExecutor device (0): NVIDIA TITAN V, Compute Capability 7.0\n",
      "I0000 00:00:1752361470.040312  121040 service.cc:160]   StreamExecutor device (1): NVIDIA TITAN V, Compute Capability 7.0\n",
      "2025-07-12 19:04:30.208415: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1752361471.432592  121040 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-07-12 19:04:34.306097: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.91GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-07-12 19:04:34.624402: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.3 = (f32[64,1,224,224]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,32,224,224]{3,2,1,0} %bitcast.15792, f32[1,32,3,3]{3,2,1,0} %bitcast.11440, f32[1]{0} %bitcast.15927), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"VQ_VAE_1/decoder_1/conv2d_19_1/convolution\" source_file=\"/home/gary/miniconda3/envs/tf_gpu/lib/python3.11/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   1/4782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30:00:56\u001b[0m 23s/step - loss: 0.2416 - recon_loss: 0.0752 - vq_loss: 0.1664"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1752361485.295720  121040 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  45/4782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20:33\u001b[0m 1s/step - loss: 0.0778 - recon_loss: 0.0179 - vq_loss: 0.0599"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------- #\n",
    "# 4.  Build, compile, train\n",
    "# --------------------------------------------------------------------- #\n",
    "if __name__ == '__main__':\n",
    "    # -------- data pipeline ------------------------------------------------\n",
    "    # `data_gen` must yield float32 tensors in [0,1] of shape (224,224,1)\n",
    "    BATCH  = 64\n",
    "    STEPS  = 100_000 // BATCH          # one epoch = entire data set\n",
    "    EPOCHS = 150\n",
    "    RELOAD = False\n",
    "    NUM_EMBEDDINGS=1024\n",
    "    MODEL_NAME=\"best.keras\"\n",
    "\n",
    "    #ds = (tf.data.Dataset.from_generator(lambda: data_gen,\n",
    "    #        output_signature=tf.TensorSpec(shape=(224, 224, 1),\n",
    "    #                                       dtype=tf.float32))\n",
    "    #      .shuffle(1000)\n",
    "    #      .batch(BATCH)\n",
    "    #     .prefetch(tf.data.AUTOTUNE))\n",
    "    if(RELOAD==True):\n",
    "        print('reloading saved model')\n",
    "        model = load_model(MODEL_NAME)\n",
    "    else:\n",
    "        print(\"building new model\")\n",
    "    # -------- build & compile ---------------------------------------------\n",
    "        model = build_vqvae(input_shape=(224,224,1),\n",
    "                            embedding_dim=32,\n",
    "                            num_embeddings=NUM_EMBEDDINGS,\n",
    "                            beta=0.25)\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(3e-4, clipnorm=1.0))\n",
    "\n",
    "    # -------- training -----------------------------------------------------\n",
    "    # Early-stopping on reconstruction loss usually works well\n",
    "    losses = ['recon_loss', 'loss', 'vq_loss']\n",
    "    es = [keras.callbacks.EarlyStopping(patience=7,\n",
    "                                       monitor=l, mode='min',\n",
    "                                       restore_best_weights=True) for l in losses]\n",
    "\n",
    "    model.fit(data_gen, epochs=20,  callbacks=[es])\n",
    "\n",
    "    print(f\"fit model with {NUM_EMBEDDINGS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7c0b46-6dd9-48f5-9c79-6e678f75ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"best.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2977b5-71b8-41ca-81e3-bf9fe6ca6976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show an example with codes and gram\n",
    "n=10\n",
    "y = model.encode(np.expand_dims(x[n],0))\n",
    "#y = model.encode(np.random.random((1,224,224,1)))\n",
    "plt.matshow(y[0])\n",
    "plt.figure()\n",
    "plt.imshow(x[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd527c23-4aa1-463a-a4f6-08a6185cbbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4660168-2913-4650-999e-12f8a5049e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21464978-669c-46ca-94fb-253af402fc04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c4ae7b-3fab-4bc6-940c-e5ca6414cd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize vq embeddings \n",
    "import keras as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from umap import UMAP\n",
    "\n",
    "model = load_model(\"best.keras\")\n",
    "vectors = model.vq_layer.embeddings.numpy()\n",
    "mapper = UMAP().fit_transform(vectors)\n",
    "plt.scatter(mapper[:,0], mapper[:,1], s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c3ccec-fcd6-4ea5-be83-338efd2e093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a46f91-1e56-4af7-ad84-84edb38beacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3bb0b2-5f44-4618-8fe3-5894ac5a9add",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42f0a92-7235-4cf0-8c0e-20ab4af14851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
